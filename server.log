INFO:     llm_client.py:35    - âœ… LLM client initialized with model: gpt-4o
INFO:     cust_logger.py:68    - Set message color for graph.py to MAGENTA
INFO:     cust_logger.py:68    - Set message color for server.py to PURPLE
INFO:     server.py:53    - {"timestamp": "2025-12-01T19:13:12.180068", "uuid": null, "received": {"uuid": "earlier-plate-compound-shop", "init": true}}
INFO:     server.py:64    - {"timestamp": "2025-12-01T19:13:12.180518", "uuid": "earlier-plate-compound-shop", "op": "Initializing ws with client."}
INFO:     server.py:53    - {"timestamp": "2025-12-01T19:13:46.873630", "uuid": "earlier-plate-compound-shop", "received": {"uuid": "earlier-plate-compound-shop", "message": "Tell me more about incident 415", "init": false}}
INFO:     graph.py:118   - ğŸš€ Using enhanced workflow for: Tell me more about incident 415...
INFO:     workflow.py:179   - ğŸš€ Processing: 'Tell me more about incident 415'
INFO:     workflow.py:73    - ğŸ¯ Orchestrator: Starting workflow
INFO:     mcp_client.py:251   - âœ… Created MCP client for session default
INFO:     mcp_client.py:190   - ğŸ“‹ Fetching available MCP tools
INFO:     mcp_client.py:56    - âœ… Fetched 47 MCP tools from server
INFO:     workflow.py:80    - ğŸ“‹ Loaded 47 available MCP tools
INFO:     orchestrator.py:36    - ğŸ¯ Orchestrator validating workflow for session earlier-plate-compound-shop
INFO:     orchestrator.py:41    - ğŸ” State validation: âœ… PASSED
INFO:     orchestrator.py:55    - âœ… Orchestrator validation passed
INFO:     workflow.py:99    - ğŸ” Query Analysis: Analyzing user query
INFO:     query_analysis_agent.py:26    - ğŸ” Analyzing query: 'Tell me more about incident 415...'
INFO:     llm_client.py:117   - ğŸ” Query analyzed: incident_analysis with confidence 0.9
INFO:     llm_client.py:260   - ğŸ“‹ Tool plan created: 1 tools planned
INFO:     query_analysis_agent.py:44    - ğŸ“‹ Tool plan: [
  {
    "name": "get_incident_by_id",
    "parameters": {
      "incident_id": "415"
    }
  }
]
INFO:     query_analysis_agent.py:64    - âœ… Query analyzed: type=incident_analysis, confidence=0.9
INFO:     workflow.py:104   - ğŸ› ï¸ Tool Execution: Executing MCP tools
INFO:     tool_execution_agent.py:37    - ğŸ› ï¸ Executing 1 tools
INFO:     tool_execution_agent.py:92    - ğŸ”§ Executing get_incident_by_id (attempt 1/2)
INFO:     mcp_client.py:251   - âœ… Created MCP client for session earlier-plate-compound-shop
INFO:     mcp_client.py:87    - ğŸ”§ Executing MCP tool: get_incident_by_id
INFO:     mcp_client.py:93    - âœ… Tool get_incident_by_id executed successfully
INFO:     tool_execution_agent.py:60    - âœ… Tool execution completed: 100.00% success rate
INFO:     workflow.py:123   - âœ¨ Response Enrichment: Enriching response
INFO:     response_enrichment_agent.py:26    - âœ¨ Enriching response
INFO:     response_enrichment_agent.py:30    - ğŸ¤– Calling LLM to generate enriched response...
INFO:     response_enrichment_agent.py:35    - ğŸŒŠ Using streaming mode for response generation
INFO:     llm_client.py:294   - ğŸ”§ Preprocessing tool: get_incident_by_id, result keys: ['incident', 'timestamp', 'tool', 'parameters', 'success']
INFO:     llm_client.py:582   - âš ï¸ No preprocessing applied for tool: get_incident_by_id
INFO:     llm_client.py:619   - Processing 1 successful tool results with search_terms: ['incident', '415']
INFO:     llm_client.py:624   - ğŸ“Š Preprocessed tool data size: 1642 bytes (1.6 KB)
INFO:     llm_client.py:641   - ğŸ“Š Context JSON size: 2554 bytes (2.5 KB)
INFO:     llm_client.py:642   - ğŸ“Š Estimated tokens: ~638.5 (rough estimate)
INFO:     llm_client.py:742   - Invoking LLM for response generation...
INFO:     llm_client.py:752   - Streaming completed, total length: 722
INFO:     response_enrichment_agent.py:41    - ğŸ” LLM response type: <class 'dict'>, value: {'final_response': 'Here is the detailed information about incident 415:\n\nâ€¢ **ID**: 415\nâ€¢ **Title**: GCP Services are down\nâ€¢ **Severity**: High\nâ€¢ **Status**: New\nâ€¢ **Description**: GCP Services 
INFO:     response_enrichment_agent.py:54    - ğŸ“ LLM generated response: 722 chars
INFO:     response_enrichment_agent.py:86    - âœ… Response enrichment completed with 5 forward links
INFO:     workflow.py:138   - ğŸ¯ Orchestrator: Finalizing workflow
INFO:     workflow.py:220   - âœ… Processing completed successfully
INFO:     graph.py:141   - {"timestamp": "2025-12-01T19:13:56.674143", "uuid": "earlier-plate-compound-shop", "llm_method": "enhanced_workflow", "sent": "Here is the detailed information about incident 415:\n\n\u2022 **ID**: 415\n\u2022 **Title**: GCP Services are do"}
INFO:     server.py:53    - {"timestamp": "2025-12-01T19:14:08.150387", "uuid": "earlier-plate-compound-shop", "received": {"uuid": "earlier-plate-compound-shop", "message": "What was the severity of those incidents?", "init": false}}
INFO:     graph.py:118   - ğŸš€ Using enhanced workflow for: What was the severity of those incidents?...
INFO:     workflow.py:179   - ğŸš€ Processing: 'What was the severity of those incidents?'
INFO:     workflow.py:193   - ğŸ“š Found existing conversation state for thread: earlier-plate-compound-shop
INFO:     workflow.py:197   - ğŸ’¬ Loaded 2 previous messages from conversation history
INFO:     workflow.py:209   - âœ… Preserved 2 messages from previous conversation
INFO:     workflow.py:73    - ğŸ¯ Orchestrator: Starting workflow
INFO:     mcp_client.py:190   - ğŸ“‹ Fetching available MCP tools
INFO:     workflow.py:80    - ğŸ“‹ Loaded 47 available MCP tools
INFO:     orchestrator.py:36    - ğŸ¯ Orchestrator validating workflow for session earlier-plate-compound-shop
INFO:     orchestrator.py:41    - ğŸ” State validation: âœ… PASSED
INFO:     orchestrator.py:55    - âœ… Orchestrator validation passed
INFO:     workflow.py:99    - ğŸ” Query Analysis: Analyzing user query
INFO:     query_analysis_agent.py:26    - ğŸ” Analyzing query: 'What was the severity of those incidents?...'
INFO:     llm_client.py:117   - ğŸ” Query analyzed: incident_analysis with confidence 0.9
INFO:     llm_client.py:260   - ğŸ“‹ Tool plan created: 2 tools planned
INFO:     query_analysis_agent.py:44    - ğŸ“‹ Tool plan: [
  {
    "name": "search_incidents",
    "parameters": {
      "query": "severity incidents"
    }
  },
  {
    "name": "get_incidents",
    "parameters": {}
  }
]
INFO:     query_analysis_agent.py:64    - âœ… Query analyzed: type=incident_analysis, confidence=0.9
INFO:     workflow.py:104   - ğŸ› ï¸ Tool Execution: Executing MCP tools
INFO:     tool_execution_agent.py:37    - ğŸ› ï¸ Executing 2 tools
INFO:     tool_execution_agent.py:92    - ğŸ”§ Executing search_incidents (attempt 1/2)
INFO:     mcp_client.py:87    - ğŸ”§ Executing MCP tool: search_incidents
INFO:     mcp_client.py:93    - âœ… Tool search_incidents executed successfully
INFO:     tool_execution_agent.py:92    - ğŸ”§ Executing get_incidents (attempt 1/2)
INFO:     mcp_client.py:87    - ğŸ”§ Executing MCP tool: get_incidents
INFO:     mcp_client.py:93    - âœ… Tool get_incidents executed successfully
INFO:     tool_execution_agent.py:60    - âœ… Tool execution completed: 100.00% success rate
INFO:     workflow.py:123   - âœ¨ Response Enrichment: Enriching response
INFO:     response_enrichment_agent.py:26    - âœ¨ Enriching response
INFO:     response_enrichment_agent.py:30    - ğŸ¤– Calling LLM to generate enriched response...
INFO:     response_enrichment_agent.py:35    - ğŸŒŠ Using streaming mode for response generation
INFO:     llm_client.py:294   - ğŸ”§ Preprocessing tool: search_incidents, result keys: ['incidents', 'count', 'page', 'page_size', 'timestamp', 'tool', 'parameters', 'success']
INFO:     llm_client.py:582   - âš ï¸ No preprocessing applied for tool: search_incidents
INFO:     llm_client.py:294   - ğŸ”§ Preprocessing tool: get_incidents, result keys: ['incidents', 'count', 'count_is_exact', 'is_limited', 'timestamp', 'tool', 'parameters', 'success']
INFO:     llm_client.py:407   - ğŸš¨ Found 41 incidents
INFO:     llm_client.py:411   - ğŸ” Applying fuzzy matching with terms: ['severity', 'incidents']
INFO:     llm_client.py:423   - ğŸ¯ After fuzzy matching, top incident scores: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
INFO:     llm_client.py:428   - âœ… Processing 10 incidents
INFO:     llm_client.py:619   - Processing 2 successful tool results with search_terms: ['severity', 'incidents']
INFO:     llm_client.py:624   - ğŸ“Š Preprocessed tool data size: 3312 bytes (3.2 KB)
INFO:     llm_client.py:641   - ğŸ“Š Context JSON size: 5964 bytes (5.8 KB)
INFO:     llm_client.py:642   - ğŸ“Š Estimated tokens: ~1491.0 (rough estimate)
INFO:     llm_client.py:736   - ğŸ“œ Including 2 previous messages in context
INFO:     llm_client.py:742   - Invoking LLM for response generation...
INFO:     llm_client.py:752   - Streaming completed, total length: 742
INFO:     response_enrichment_agent.py:41    - ğŸ” LLM response type: <class 'dict'>, value: {'final_response': 'Certainly! Here\'s an enriched response with detailed information about incident 415:\n\nIncident 415 is a high-severity issue titled "GCP Services are down." This incident was tri
INFO:     response_enrichment_agent.py:54    - ğŸ“ LLM generated response: 742 chars
INFO:     response_enrichment_agent.py:86    - âœ… Response enrichment completed with 5 forward links
INFO:     workflow.py:138   - ğŸ¯ Orchestrator: Finalizing workflow
INFO:     workflow.py:220   - âœ… Processing completed successfully
INFO:     graph.py:141   - {"timestamp": "2025-12-01T19:14:20.085691", "uuid": "earlier-plate-compound-shop", "llm_method": "enhanced_workflow", "sent": "Certainly! Here's an enriched response with detailed information about incident 415:\n\nIncident 415 i"}
ERROR:     server.py:73    - {"timestamp": "2025-12-01T19:16:56.979280", "uuid": "earlier-plate-compound-shop", "op": "Error: (<CloseCode.ABNORMAL_CLOSURE: 1006>, '')"}
INFO:     server.py:77    - {"timestamp": "2025-12-01T19:16:56.979648", "uuid": "earlier-plate-compound-shop", "op": "Closing connection."}
ERROR:     server.py:82    - {"timestamp": "2025-12-01T19:16:56.979769", "uuid": "earlier-plate-compound-shop", "op": "WebSocket close error: Unexpected ASGI message 'websocket.close', after sending 'websocket.close' or response already completed."}
INFO:     server.py:53    - {"timestamp": "2025-12-01T19:17:00.181352", "uuid": null, "received": {"uuid": "earlier-plate-compound-shop", "init": true}}
INFO:     server.py:64    - {"timestamp": "2025-12-01T19:17:00.181648", "uuid": "earlier-plate-compound-shop", "op": "Initializing ws with client."}
