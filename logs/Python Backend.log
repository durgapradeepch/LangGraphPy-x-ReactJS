INFO:     llm_client.py:35    - âœ… LLM client initialized with model: gpt-4o
INFO:     cust_logger.py:68    - Set message color for graph.py to MAGENTA
INFO:     cust_logger.py:68    - Set message color for server.py to PURPLE
INFO:     Started server process [44842]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:65504 - "WebSocket /ws" [accepted]
INFO:     connection open
INFO:     server.py:53    - {"timestamp": "2025-12-01T16:52:32.990947", "uuid": null, "received": {"uuid": "ice-even-planning-became", "init": true}}
INFO:     server.py:64    - {"timestamp": "2025-12-01T16:52:32.991217", "uuid": "ice-even-planning-became", "op": "Initializing ws with client."}
ERROR:     server.py:73    - {"timestamp": "2025-12-01T16:52:35.978250", "uuid": "ice-even-planning-became", "op": "Error: (1001, '')"}
INFO:     server.py:77    - {"timestamp": "2025-12-01T16:52:35.978567", "uuid": "ice-even-planning-became", "op": "Closing connection."}
ERROR:     server.py:82    - {"timestamp": "2025-12-01T16:52:35.978639", "uuid": "ice-even-planning-became", "op": "WebSocket close error: Unexpected ASGI message 'websocket.close', after sending 'websocket.close' or response already completed."}
INFO:     connection closed
INFO:     127.0.0.1:65515 - "WebSocket /ws" [accepted]
INFO:     connection open
INFO:     server.py:53    - {"timestamp": "2025-12-01T16:52:36.102205", "uuid": null, "received": {"uuid": "quick-chart-trap-writer", "init": true}}
INFO:     server.py:64    - {"timestamp": "2025-12-01T16:52:36.102390", "uuid": "quick-chart-trap-writer", "op": "Initializing ws with client."}
INFO:     server.py:53    - {"timestamp": "2025-12-01T16:52:45.604519", "uuid": "quick-chart-trap-writer", "received": {"uuid": "quick-chart-trap-writer", "message": "Show pods stuck in CrashLoopBackOff", "init": false}}
INFO:     graph.py:171   - {"timestamp": "2025-12-01T16:52:51.291326", "uuid": "quick-chart-trap-writer", "llm_method": "on_chat_model_end", "sent": "To identify pods that are stuck in the `CrashLoopBackOff` state in a Kubernetes cluster, you can use the `kubectl` command-line tool. Here\u2019s how you can do it:\n\n1. **List all pods and filter by status:**\n\n   You can use the following command to list all pods across all namespaces that are in the `CrashLoopBackOff` state:\n\n   ```bash\n   kubectl get pods --all-namespaces --field-selector=status.phase!=Running -o wide | grep CrashLoopBackOff\n   ```\n\n   This command lists all pods that are not in the `Running` phase and then filters the output to show only those in the `CrashLoopBackOff` state.\n\n2. **Using JSONPath for more precise filtering:**\n\n   Alternatively, you can use JSONPath to filter and display only the pods in `CrashLoopBackOff`:\n\n   ```bash\n   kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}{\"\\t\"}{.metadata.name}{\"\\t\"}{.status.containerStatuses[*].state.waiting.reason}{\"\\n\"}{end}' | grep CrashLoopBackOff\n   ```\n\n   This command extracts the namespace, pod name, and the reason for the waiting state, then filters for `CrashLoopBackOff`.\n\n3. **Describe the pod for more details:**\n\n   Once you identify a pod in `CrashLoopBackOff`, you can get more details about why it is in this state by using the `describe` command:\n\n   ```bash\n   kubectl describe pod <pod-name> -n <namespace>\n   ```\n\n   Replace `<pod-name>` and `<namespace>` with the actual pod name and namespace. This command provides detailed information about the pod, including events and error messages that can help diagnose the issue.\n\n4. **Check pod logs:**\n\n   To further investigate, you can check the logs of the pod to understand why it is crashing:\n\n   ```bash\n   kubectl logs <pod-name> -n <namespace> --previous\n   ```\n\n   The `--previous` flag is useful if the pod has restarted and you want to see the logs from the previous instance.\n\nBy following these steps, you should be able to identify and troubleshoot pods that are stuck in the `CrashLoopBackOff` state."}
start.sh: line 56: 44842 Killed: 9               uvicorn server:app --host 0.0.0.0 --port 8000
